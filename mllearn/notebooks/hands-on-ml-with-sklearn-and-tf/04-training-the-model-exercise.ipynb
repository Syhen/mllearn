{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 如果你有一个数百万特征的训练集，你应该选择哪种线性回归训练算法？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: 随机梯度下降或者小批量梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 假设你的训练集中特征的数据尺度（scale）有着非常大的差异，哪种算法会受到影响？有多大的影响？对于这些影响你可以做什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T06:59:13.434526Z",
     "start_time": "2018-08-29T06:59:13.413504Z"
    }
   },
   "source": [
    "A: 梯度下降算法会受到影响。速度可能会变得很慢。在使用梯度下降之前，首先将数据约束到有相近的尺度范围。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 训练Logistic回归模型时，梯度下降是否会陷入局部最低点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: 不会。因为Logistic回归模型的损失函数是一个凸函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 在有足够的训练时间下，是否所有的梯度下降都会得到相同的模型参数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: 不会。因为随机梯度下降和小批量梯度下降损失值最终会在局部最小值附近震荡，而不是确定的一个值。所以模型参数不会都相同，但是很接近。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 假设你使用批量梯度下降法，画出每一代的验证误差。当你发现验证误差一直增大，接下来会发生什么？你怎么解决这个问题？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T07:03:35.920760Z",
     "start_time": "2018-08-29T07:03:35.916097Z"
    }
   },
   "source": [
    "A: 接下来误差会越来越大。应该尝试：减小学习率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 当验证误差升高时，立即停止小批量梯度下降是否是一个好主意？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: 不是的。因为小批量提取下降误差变化的曲线不是平滑的，会有局部的震荡。一个好的办法是：当一段时间后，验证误差一直在升高或变化很小时，停止训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 哪个梯度下降算法（我们讨论的那些算法中）可以最快到达解的附近？哪个的确实会收敛？怎么使其他算法也收敛？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: 小批量梯度下降算法会最快到达解的附近。批量梯度下降会收敛。要使得别的算法也收敛，可以根据动态的减小学习率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T07:07:21.204023Z",
     "start_time": "2018-08-29T07:07:21.200738Z"
    }
   },
   "source": [
    "# 练习8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 假设你使用多项式回归，画出学习曲线，在图上发现学习误差和验证误差之间有着很大的间隙。这表示发生了什么？有哪三种方法可以解决这个问题？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: 发生了过拟合。解决办法为：1. 搜集更多的数据扩充训练集；2. 对模型使用正则化；3. 使用更加简单的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 假设你使用岭回归，并发现训练误差和验证误差都很高，并且几乎相等。你的模型表现是高偏差还是高方差？这时你应该增大正则化参数$\\alpha$，还是降低它？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: 高偏差，也就是欠拟合。应该减小正则化参数，这样会提高模型的自由度，让模型能够更好的拟合数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 你为什么要这样做：\n",
    "    - 使用岭回归代替线性回归？\n",
    "    - Lasso回归代替岭回归？\n",
    "    - 弹性网络代替Lasso回归？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: 那就告诉你呗：\n",
    "    - 模型过拟合，需要使用正则化减小模型自由度\n",
    "    - 模型只有少数参数对预测有效\n",
    "    - 训练集的特征数量较大时，Lasso可能出现不规律的情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 假设你想判断一幅图片是室内还是室外，白天还是晚上。你应该选择二个逻辑回归分类器还是一个Softmax分类器？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: 二个逻辑回归分类器。因为一个Softmax尽管能够针对多类别进行输出，但是一次只能输出一个结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 在Softmax回归上应用批量梯度下降的早期停止法（不使用Scikit-Learn）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: 见以下代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用sklearn获取数据(iris)，并进行train-test分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T10:15:47.214339Z",
     "start_time": "2018-08-29T10:15:46.073877Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先加载iris数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T10:15:47.389320Z",
     "start_time": "2018-08-29T10:15:47.382432Z"
    }
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在多类别分类中，我们需要把标签转换成one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T10:15:48.271407Z",
     "start_time": "2018-08-29T10:15:48.266796Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot(y):\n",
    "    n_classes = len(set(y))\n",
    "    m = len(y)\n",
    "    y_one_hot = np.zeros((m, n_classes))\n",
    "    y_one_hot[range(m), y] = 1\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T10:15:48.337092Z",
     "start_time": "2018-08-29T10:15:48.318691Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot([0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看起来不错\n",
    "\n",
    "接下来，我们把标签转换成one-hot编码，并且分割数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T10:15:49.470718Z",
     "start_time": "2018-08-29T10:15:49.464349Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (120, 4)\n",
      "y shape: (120, 3)\n"
     ]
    }
   ],
   "source": [
    "y = one_hot(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"X shape:\", X_train.shape)\n",
    "print(\"y shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一些函数，包括softmax, predict, error_function, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T10:15:50.410697Z",
     "start_time": "2018-08-29T10:15:50.407035Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X = np.array(X)\n",
    "    return np.exp(X) / np.exp(X).sum(axis=1).reshape((X.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T10:15:50.461801Z",
     "start_time": "2018-08-29T10:15:50.456297Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09003057, 0.24472847, 0.66524096],\n",
       "       [0.01714783, 0.04661262, 0.93623955]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([[1, 2, 3], [2, 3, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T10:15:50.509446Z",
     "start_time": "2018-08-29T10:15:50.501496Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    X_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    z = X_bias.dot(theta.T)\n",
    "    return softmax(z)\n",
    "\n",
    "\n",
    "def error_log_entropy(X, y, theta):\n",
    "    y_predict = predict(X, theta)\n",
    "    n_classes = y.max() + 1\n",
    "    m = X.shape[0]\n",
    "    error = -1. / m * (np.log(y_predict) * y).sum().sum()\n",
    "    return error\n",
    "\n",
    "\n",
    "def get_gradient(X, y, y_pred, theta):\n",
    "    m = X.shape[0]\n",
    "    X_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    return 1. / m * ((y_pred - y).T.dot(X_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T10:15:50.571643Z",
     "start_time": "2018-08-29T10:15:50.563933Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333333, 0.33333333, 0.33333333],\n",
       "       [0.33333333, 0.33333333, 0.33333333]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(np.array([[1, 2, 3], [3, 4, 4]]), np.array([[0, 1, 1, 1], [0, 1, 1, 1], [0, 1, 1, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T10:15:50.617662Z",
     "start_time": "2018-08-29T10:15:50.611016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0986122886681098"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_log_entropy(X_train, y_train, np.array([[0, 1, 1, 1, 1], [0, 1, 1, 1, 1], [0, 1, 1, 1, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T10:15:50.670010Z",
     "start_time": "2018-08-29T10:15:50.661794Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.12792746e-16,  2.79111111e-01, -1.21333333e-01,\n",
       "         7.64888889e-01,  3.18222222e-01],\n",
       "       [-1.20274161e-16, -3.08888889e-02,  9.46666667e-02,\n",
       "        -1.67111111e-01, -4.24444444e-02],\n",
       "       [-4.11522668e-16, -2.48222222e-01,  2.66666667e-02,\n",
       "        -5.97777778e-01, -2.75777778e-01]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.array([[0, 1, 1, 1, 1], [0, 1, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
    "get_gradient(X, y, predict(X, theta), theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，开始我们的迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T10:15:51.816947Z",
     "start_time": "2018-08-29T10:15:51.576444Z"
    }
   },
   "outputs": [],
   "source": [
    "n_iterations = 1000\n",
    "eta = 0.1\n",
    "m, n = X_train.shape\n",
    "n_classes = y_train.shape[1]\n",
    "theta = np.concatenate([np.c_[np.ones((1, 1)), np.random.random((1, n))] for _ in range(n_classes)])\n",
    "best_loss = float(\"inf\")\n",
    "best_iter = 0\n",
    "best_theta = theta\n",
    "loss_time = 0\n",
    "MAX_LOSS_TIME = 5\n",
    "for i in range(n_iterations):\n",
    "    loss = error_log_entropy(X_train, y_train, theta)\n",
    "    y_pred = predict(X, theta)\n",
    "    gradient = get_gradient(X, y, y_pred, theta)\n",
    "    theta = theta - eta * gradient\n",
    "    loss_delta = best_loss - loss\n",
    "    if 0 < loss_delta < 0.0001:\n",
    "        loss_time += 1\n",
    "    if loss_time == MAX_LOSS_TIME:\n",
    "        break\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_iter = i\n",
    "        best_theta = theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T10:15:51.832201Z",
     "start_time": "2018-08-29T10:15:51.821286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss: 0.14990847846651215\n",
      "Best iter: 690\n",
      "Best theta: [[ 1.43986895  1.05252648  2.66766967 -2.00019001 -0.48141481]\n",
      " [ 1.49506949  1.19777522  0.27683077  0.29298624 -0.33871456]\n",
      " [ 0.06506156 -0.91480279 -0.94119927  2.97148979  2.71010066]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Best loss:\", best_loss)\n",
    "print(\"Best iter:\", best_iter)\n",
    "print(\"Best theta:\", best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T10:15:51.842431Z",
     "start_time": "2018-08-29T10:15:51.836656Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuray(X, y):\n",
    "    y_pred = np.argmax(predict(X, best_theta), axis=1)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    return (y_pred == y).sum() / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T10:15:51.853978Z",
     "start_time": "2018-08-29T10:15:51.845465Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: 0.975\n",
      "Test Set: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set:\", accuray(X_train, y_train))\n",
    "print(\"Test Set:\", accuray(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 涉及公式的推导\n",
    "2. 公式到代码快速转换的能力的培养"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
